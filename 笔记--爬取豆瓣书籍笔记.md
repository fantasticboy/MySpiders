本文使用scrapy进行爬取豆瓣所有书籍，具体的方案如下：
首先进入所有标签页，获取书籍标签；
然后进入具体标签页，得到该标签下的所有书籍；
最后进入书籍详情页，得到具体书籍信息，存入csv文件格式里。

#在终端，敲击如下命令，创建scrapy工程：
```shell
$ scrapy startproject projectname
```
(其中projectname,是自定义的项目名）
# 在项目目录下，执行下述命令：
```shell
$ scrapy genspider spidername crawlWebName
```
其中**spidername**是要在项目中生成你的主爬虫文件的名字，以后主要的爬虫逻辑都在该文件里撰写。
**crawlWebName**，则是指你要爬取网址的域名，比如book.douban.com
#主要文件，doubanbookSpider.py文件，具体内容如下：
```python
# -*- coding: utf-8 -*-
import scrapy
from doubanBook.items import DoubanbookItem

class DoubanbookspiderSpider(scrapy.Spider):
    name = 'doubanbookSpider'
    # allowed_domains = ['book.douban.com']
    start_urls = ['https://book.douban.com/tag/?view=type&icn=index-sorttags-all/']

    #进行一次解析，'https://book.douban.com/tag/?view=type&icn=index-sorttags-all/'，获得所有的书籍标签
    def parse(self, response):
        #抽取出含有<a></a>，<b></b>标签的html
        Types = response.xpath("//div[@class='article']/div[@class='']//table//td")

        for ty in Types:
            items = DoubanbookItem()
            #从已有的html语句中再次匹配抽取<a></a>中的内容
            bookType = ty.xpath(".//a/text()").extract()
            # 从已有的html语句中再次匹配抽取<b></b>中的内容
            bookNum = ty.xpath(".//b/text()").extract()

            #存入bookType，bookNum域中
            items['bookType'] = bookType[0]
            items['bookNum'] = bookNum[0]
            #将得到标签页的url，标签本身的数据（标签，数量）信息传递到second_parse解析方法里中去
            yield scrapy.Request(url = 'https://book.douban.com/tag/'+bookType[0],
                                 meta={'meta_1':items},
                                 callback=self.second_parse)
    #进行二次解析，比如被标注为‘小说’标签的书籍，（https://book.douban.com/tag/小说）
    def second_parse(self,response):
        #将传输的items里数据，进行接收
        meta_1 = response.meta['meta_1']

        books = response.xpath("//div[@class='info']/h2/a/@href").extract()
        #将书籍详情页的url，上次解析出items里的数据，传递到详情解析函数detail_parse内中
        for book in books:
            bookUrl = book
            yield scrapy.Request(url=bookUrl,
                                 meta={"meta_2":meta_1},
                                 callback=self.detail_parse)
        #如果该页内的书籍解析完成，则进行翻页，解析下一页的书籍，继续执行second_parse函数
        if len(response.xpath("//span[@class='next']/a/text()").extract())!=0:
            urlBooks = response.xpath("//span[@class='next']/a/@href").extract()
            for urlBook in urlBooks:
                yield scrapy.Request(url="https://book.douban.com"+urlBook,
                                 meta={"meta_1":meta_1},
                                 callback=self.second_parse)

    #详情页解析
    def detail_parse(self,response):
        #接收传递的items数据
        items = response.meta['meta_2']
        #书名
        bookName = response.xpath('//*[@id="wrapper"]/h1/span/text()').extract()
        #书籍封面
        bookImg = response.xpath("//div[@id='mainpic']/a/img/@src").extract()
        #作者
        if len(response.xpath("//*[@id='info']/a[1]/text()").extract())==0:
            bookAuthor = response.xpath("//div[@id='info']/span/a/text()").extract()
        else:
            bookAuthor = response.xpath("//*[@id='info']/a[1]/text()").extract()
        #出版商，匹配无标签的文本，使用following::text()方法
        bookPublisher = response.xpath("//span[./text()='出版社:']/following::text()[1]").extract()
        #出版时间
        bookTime = response.xpath("//span[./text()='出版年:']/following::text()[1]").extract()
        #书籍页数
        bookSize = response.xpath("//span[./text()='页数:']/following::text()[1]").extract()
        #书籍价格
        bookPrice = response.xpath("//span[./text()='定价:']/following::text()[1]").extract()
        #书籍ISBN号
        bookISBN = response.xpath("//span[./text()='ISBN:']/following::text()[1]").extract()
        #书籍简介
        if len(response.xpath('//*[@id="link-report"]/span[1]/div/p/text()').extract())!=0:
            bookIntros = response.xpath('//*[@id="link-report"]/span[1]/div/p/text()').extract()
        else:
            bookIntros = response.xpath("//*[@id='link-report']/div[1]/div/p/text()").extract()
        #由于书籍简介有多行p标签构成，所以使用for循环将其拼接起来
        bookIntro = ""
        for bookIntro_one in bookIntros:
            bookIntro += bookIntro_one
        #strip()是为了去除字符串中无空白字符或者是无用字符
        items['bookName'] = bookName[0].strip()
        items['bookImg'] = bookImg[0].strip()
        items['bookAuthor'] = bookAuthor[0].strip()
        items['bookPublisher'] = bookPublisher[0].strip()
        items['bookTime'] = bookTime[0].strip()
        items['bookSize'] = bookSize[0].strip()
        items['bookPrice'] = bookPrice[0].strip()
        items['bookISBN'] = bookISBN[0].strip()
        items['bookIntro'] = bookIntro

        yield items
```
#items.py文件
```python
# -*- coding: utf-8 -*-

# Define here the models for your scraped items
#
# See documentation in:
# https://doc.scrapy.org/en/latest/topics/items.html

import scrapy
class DoubanbookItem(scrapy.Item):
    # define the fields for your item here like:
    # name = scrapy.Field()
    bookType = scrapy.Field()
    bookNum = scrapy.Field()

    bookImg = scrapy.Field()
    bookName = scrapy.Field()
    bookAuthor = scrapy.Field()
    bookPublisher = scrapy.Field()
    bookTime = scrapy.Field()
    bookPrice = scrapy.Field()
    bookISBN = scrapy.Field()
    bookSize = scrapy.Field()
    bookIntro = scrapy.Field()
```
#pipelines.py文件
```python
# -*- coding: utf-8 -*-

# Define your item pipelines here
#
# Don't forget to add your pipeline to the ITEM_PIPELINES setting
# See: https://doc.scrapy.org/en/latest/topics/item-pipeline.html

import csv
import codecs

class DoubanbookPipeline(object):

    colname = ['bookType','bookNum',
               'bookImg','bookName',
               'bookAuthor','bookPublisher',
               'bookTime','bookPrice',
               'bookPrice','bookISBN',
               'bookSize','bookIntro']

    def __init__(self):
        #在爬虫启动时，创建csv,并设置newline=''来避免空行
        self.file = open('输出.csv','w',encoding='utf-8',newline='')
        #启动csv的字典写入方法
        self.writer = csv.DictWriter(self.file,self.colname)
        #写入字段名称作为首行
        self.writer.writeheader()

    def process_item(self,item,spider):
        #把每次输出的item，写入csv中
        self.writer.writerow(item)
        return item

    def close_spider(self,spider):
        #在爬虫结束时，关闭文件节省资源
        self.file.close()
```
#setting.py文件
```python
# -*- coding: utf-8 -*-

# Scrapy settings for doubanBook project
#
# For simplicity, this file contains only settings considered important or
# commonly used. You can find more settings consulting the documentation:
#
#     https://doc.scrapy.org/en/latest/topics/settings.html
#     https://doc.scrapy.org/en/latest/topics/downloader-middleware.html
#     https://doc.scrapy.org/en/latest/topics/spider-middleware.html

BOT_NAME = 'doubanBook'

SPIDER_MODULES = ['doubanBook.spiders']
NEWSPIDER_MODULE = 'doubanBook.spiders'


# Crawl responsibly by identifying yourself (and your website) on the user-agent
USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Safari/537.36'

# Obey robots.txt rules
# ROBOTSTXT_OBEY = True

# Configure maximum concurrent requests performed by Scrapy (default: 16)
#CONCURRENT_REQUESTS = 32

# Configure a delay for requests for the same website (default: 0)
# See https://doc.scrapy.org/en/latest/topics/settings.html#download-delay
# See also autothrottle settings and docs
#DOWNLOAD_DELAY = 3
# The download delay setting will honor only one of:
#CONCURRENT_REQUESTS_PER_DOMAIN = 16
#CONCURRENT_REQUESTS_PER_IP = 16

# Disable cookies (enabled by default)
COOKIES_ENABLED = False

# Disable Telnet Console (enabled by default)
#TELNETCONSOLE_ENABLED = False

# Override the default request headers:
#DEFAULT_REQUEST_HEADERS = {
#   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
#   'Accept-Language': 'en',
#}

# Enable or disable spider middlewares
# See https://doc.scrapy.org/en/latest/topics/spider-middleware.html
#SPIDER_MIDDLEWARES = {
#    'doubanBook.middlewares.DoubanbookSpiderMiddleware': 543,
#}

# Enable or disable downloader middlewares
# See https://doc.scrapy.org/en/latest/topics/downloader-middleware.html
#DOWNLOADER_MIDDLEWARES = {
#    'doubanBook.middlewares.DoubanbookDownloaderMiddleware': 543,
#}

# Enable or disable extensions
# See https://doc.scrapy.org/en/latest/topics/extensions.html
#EXTENSIONS = {
#    'scrapy.extensions.telnet.TelnetConsole': None,
#}

# Configure item pipelines
# See https://doc.scrapy.org/en/latest/topics/item-pipeline.html
ITEM_PIPELINES = {
   'doubanBook.pipelines.DoubanbookPipeline': 300,
}

# Enable and configure the AutoThrottle extension (disabled by default)
# See https://doc.scrapy.org/en/latest/topics/autothrottle.html
#AUTOTHROTTLE_ENABLED = True
# The initial download delay
#AUTOTHROTTLE_START_DELAY = 5
# The maximum download delay to be set in case of high latencies
#AUTOTHROTTLE_MAX_DELAY = 60
# The average number of requests Scrapy should be sending in parallel to
# each remote server
#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0
# Enable showing throttling stats for every response received:
#AUTOTHROTTLE_DEBUG = False

# Enable and configure HTTP caching (disabled by default)
# See https://doc.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings
# HTTPCACHE_ENABLED = True
# HTTPCACHE_EXPIRATION_SECS = 0
# HTTPCACHE_DIR = 'httpcache'
# HTTPCACHE_IGNORE_HTTP_CODES = []
# HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'
```
#问题与解决办法
###1.有可能被网站识别为不能爬虫，不能抓取。
解决方法：要在setting文件中，添加USER_AGENT，如setting文件内所示。USER_AGENT，可以从
[浏览器请求头大全中得到](http://www.useragentstring.com/pages/useragentstring.php?name=Chrome)
###2.生成的csv文件，使用Excel打开时，乱码。
解决方法：将csv文件，保存为GBK编码就可以了。
###3.生成的csv文件，无序
解决办法如下：
CsvItemExporter.py文件---用于按自定义顺序保存
```python
from scrapy.conf import settings
from scrapy.contrib.exporter import CsvItemExporter
class ProjectCsvItemExporter(CsvItemExporter):
    def __init__(self, *args, **kwargs):
        kwargs['delimiter'] = settings.get('CSV_DELIMITER', ',')
        if settings.get('FIELDS_TO_EXPORT', []):
            kwargs['fields_to_export'] = settings.get('FIELDS_TO_EXPORT', [])
        super(ProjectCsvItemExporter, self).__init__(*args, **kwargs)
```
需要在setting文件末尾，添加如下代码，即可完成相应的自定义保存顺序
```python
#调整输出顺序
FEED_EXPORTERS = {
    'csv':'doubanBook.CsvItemExporter.ProjectCsvItemExporter'
}
#导出字段顺序
FIELDS_TO_EXPORT = [
    'bookType',
    'bookNum',
    'bookImg',
    'bookName',
    'bookAuthor',
    'bookPublisher',
    'bookTime',
    'bookPrice',
    'bookISBN',
    'bookSize',
    'bookIntro'
]
```



# 小技巧

1.使用Xpath Helper插件，可以提前验证自己的xpath表达式是否正确

2.也可以在Chrome的检查工具里，右键点击选中要匹配的HTML文本，直接copy xpath表达式

#参考blog

#####[http://dmcoders.com/2017/08/04/python-scrapy/](http://dmcoders.com/2017/08/04/python-scrapy/)
#####[https://lizonghang.github.io/2016/07/05/Scrapy%E9%87%87%E9%9B%86%E5%A4%A9%E6%B0%94%E6%95%B0%E6%8D%AE/](https://lizonghang.github.io/2016/07/05/Scrapy%E9%87%87%E9%9B%86%E5%A4%A9%E6%B0%94%E6%95%B0%E6%8D%AE/)
#####[https://blog.csdn.net/HHTNAN/article/details/74199748](https://blog.csdn.net/HHTNAN/article/details/74199748)











